---
title: "p8105_hw3_lcc2181"
output: github_document
---

Loading packages needed for Homework 3 and adding code to format plots.

```{r, message=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

## Problem 1

Loading instacart dataset for Problem 1

```{r}
data("instacart")
```

There are `r nrow(instacart)` observations and `r ncol(instacart)` variables in the Instacart dataset. Some key variables in the data set include `order_id`, `product_id`, `order_dow`, `product_name`, and `aisle`. `order_dow` describes what day of the week the order was placed. `product_name` describes the name of the product and `aisle` describes the name of the aisle. For example, the first observation in the dataset contains an order for Bulgarian Yogurt from the yogurt aisle.


How many aisles are there and which aisles are the most items ordered from?

*Using the `count` function to count the number of aisle_id's in the instacart data set, and the `distinct` function to remove duplicate aisle_ids to ensure all aisle_id's being counted are unique.
*Using the `group_by` function to group by aisle_id and `summarize` to summarize the total number of observations by aisle_id, assuming each observation in the dataset represents an item ordered. 
*Using `arrrange` to order in descending order, so that aisles will be listed in order most to least items ordered.

```{r}
count(distinct(instacart, aisle_id)) 

instacart %>% 
  group_by(aisle_id) %>% 
  summarize(number_of_items = n()) %>% 
  arrange(desc(number_of_items))

```

*There are 134 aisles. 
*The most items are ordered from aisle 83 (150609) followed by aisle 24 (150473).

Making a plot showing number of items ordered in each aisle. The plot is limited to aisles with more than 10000 items ordered.

*Using `group_by` to group by aisle_id and `summarize` to count the total number of observations by aisle_id, assuming each observation (row) represents an item ordered
*Using `filter` to restrict to aisles with more than 10,0000 items ordered
*Using `ggplot` to create a plot with aisle_id on the x-axis and number_of_items on the y-axis

```{r}
instacart %>% 
  group_by(aisle_id) %>% 
  summarize(number_of_items = n()) %>% 
  filter(number_of_items > 10000) %>% 
  ggplot(aes(x = aisle_id, y = number_of_items)) +
  geom_point()
```

Making a table showing the three most popular items in each of the aisles.

*Using `filter` to filter the data set to the three aisles - "baking ingredients", "dog food care", and "packaged vegetables fruits"
*Using `group_by` to group the observations by aisle and product_name
*Using `summarize` to count the total number of observations by aisle_id, assuming each observation (row) represents an item ordered
*Using `filter` to filter to those with a ranking of less than 4 (so, a ranking of 1, 2, or 3) as determined by using the `min_rank` function with descending n_items to ensure that those most number of items are ranked as highest
*Using `select` to ensure that the variables aisle, product_name, and n_items are included in the table
*Using `arrange` to order the table by aisle and most to least number of items in each aisle
*Using `knitr::kable()` to convert the table into a reader-friendly format

```{r}
instacart %>% 
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>% 
  summarize(n_items = n()) %>% 
  filter(min_rank(desc(n_items)) < 4) %>% 
  select(aisle, product_name, n_items) %>% 
  arrange(aisle, desc(n_items)) %>% 
  knitr::kable()
```

Making a table showing mean hour of day at which Pink Lady Apples and Coffee Ice Cream are ordered each day of week

*Using `filter` to limit the table to the products "Pink Lady Apples" and "Coffee Ice Cream"
*Using `mutate` to convert the order_dow variable into day names rather than numbers using the `recode` function, assuming that Sunday represents day 0.
*Using `group_by` to group by order_dow and product_name
*Using `summarize` to create a new variable mean_hour_of_day which takes the mean value of the order hour of day
*Using `pivot_wider` to convert format of table so that order_dow becomes columns taking on values from mean_hour_of_day variable created in the previous step
*Using `relocate` to order the table with product_name first followed by days of the week in chronological order
*Using `knitr::kable()` to convert the table into a reader-friendly format

```{r, message=FALSE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  mutate(order_dow = recode(order_dow,
    `0` = "Sunday", `1` = "Monday", `2` = "Tuesday", `3` = "Wednesday", 
    `4` = "Thursday", `5` = "Friday", `6` = "Saturday")) %>% 
  group_by(order_dow, product_name) %>%
  summarize(mean_hour_of_day = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_of_day
  ) %>%
  relocate(product_name, Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday) %>% 
  knitr::kable()
```

## Problem 2

Loading the BRFSS data for Problem 2

```{r}
data("brfss_smart2010")
```

Cleaning the BRFSS data and assigning to new data frame `brfss_df`:

*Using `janitor::clean_names()` to format the variable names appropriately
*Using `rename` to change the variable name "locationabbr" to "state" and "locationdesc" to "location"
*Using `filter` to focus only on the "Overall Health" topic
*Using `mutate` to convert response into a factor variable with 5 levels ordered from "Poor" to "Excellent"

```{r}
brfss_df =
  brfss_smart2010 %>% 
  janitor::clean_names() %>%
  rename(state = locationabbr, location = locationdesc) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, 
      levels = c("Poor", "Fair", "Good", "Very good", "Excellent")), 
      ordered = TRUE
  )
```

In 2002, which states were observed at 7 or more locations? What about in 2010?

*Using `filter` to filter to the correct year (2002 in the first code block and 2010 in the second)
*Using `group_by` to group by state and `summarize` to count location the number of distinct locations
*Using `filter` to filter to states with 7 or more locations

```{r}
brfss_df %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n_locations = n_distinct(location)) %>% 
  filter(n_locations >= 7)

brfss_df %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  summarize(n_locations = n_distinct(location)) %>% 
  filter(n_locations >= 7)
```

*In 2002, the states with 7 or more locations were CT, FL, MA, NC, NJ, and PA
*In 2010, the states with 7 or more locations were CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA

Constructing a data set limited to "Excellent" responses, containing year, state, and a variable that averages data_value across locations within state. 
Making a spaghetti plot of this average value of time within a state.

*Filtering to only include response values of "Excellent"
*Using `group_by` to group by state and year and `summarize` to create a new variable avg_data_value that takes the mean data_value across locations within a state in a given year
*Creating `ggplot` with year on x-axis and avg_data_value on y-axis and grouping by state to show a different line for each state

```{r}
brfss_excellent =
  brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  summarize(avg_data_value = mean(data_value))

brfss_excellent %>% 
  ggplot(aes(x = year, y = avg_data_value, group = state)) +
  geom_line()
```

Making a two-panel plot (for the years 2006 and 2010) showing distribution of data_value for responses among locations in NY state

*Using `filter` to filter to only NY state and to include either years 2006 or 2010
*Creating `ggplot` with response on x-axis and data_value on y_axis, adding `geom_point()` to include points for each location, and adding `facet_grid` to split into a two-panel plot by year (2006 and 2010)

```{r}
brfss_df %>% 
  filter(state == "NY", year %in% c(2006, 2010)) %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_point() +
  facet_grid(. ~ year)
```

## Problem 3

Loading, tidying and wrangling the data for use in Problem 3

*Using `read_csv` to load in the data file
*Including a weekday vs. weekend variable by using `mutate` to create new variable weekday_vs_weekend which is created using `recode` to code days of the week
*Using `pivot_longer` to display activity count at each minute as a new observation (row) rather than as a variable (column)
*Using mutate to convert new `minute` variable to numeric

```{r}
accel_df =
  read_csv("data/accel_data.csv", show_col_types = FALSE) %>%
  mutate(
    weekday_vs_weekend = recode(day,
      "Monday" = "Weekday", "Tuesday" = "Weekday",  "Wednesday" = "Weekday",
      "Thursday" = "Weekday",  "Friday" = "Weekday", "Saturday" = "Weekend", 
      "Sunday" = "Weekend")
  ) %>% 
  pivot_longer (
    activity.1:activity.1440,
    names_to = "minute",
    names_prefix = "activity.",
    values_to = "activity"
  ) %>% 
  mutate(minute = as.numeric(minute))

```
The resulting dataset has `r nrow(accel_df)` observations and `r ncol(accel_df)` columns. The variables are week, day_id, day, weekday_vs_weekend, minute, and activity. There is an observation recording activity counts for each minute of each day over the course of 35 days.


Creating a total activity variable for each day, aggregating across minutes
*Using `group_by` to group by day and `summarize` to create a new variable total_activity which is sum of all activity observations from each minute of the day.

```{r}
accel_df %>% 
  group_by(day_id) %>% 
  summarize(total_activity = sum(activity))
```

The individual's activity counts vary substantially by day. His highest activity count is reported on day 16 (685910.00) and his lowest total activity count is reported on days 24 and 31 (1440.00).

Making a single-panel plot that shows the 24-hour activity time courses each day, using color to indicate day of the week.
*Using `mutate` to divide the minute variable by 60 in order to convert minutes into hours
*Creating a `ggplot` with hour on the x-axis and activity on the y-axis, colored by day of the week; adding `geom_smooth()` to create a line for each day.

```{r}
accel_df %>% 
  mutate(hour = minute/60) %>% 
  ggplot(aes(x = hour, y = activity, group = day_id, color = day)) +
  geom_line() +
  labs(
    title = "24 Hour Activity Time Course For Each Day",
    x = "Hour of the Day",
    y = "Activity"
  ) +
  scale_x_continuous(breaks = c(0, 4, 8, 12, 16, 20, 24))
```
Based on this graph, I can tell the day of week and hour of day the individual had the highest activity count. Specifically, the individual appears to have been most active on a Wednesday in the 19th hour of the day.