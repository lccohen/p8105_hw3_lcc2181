---
title: "p8105_hw3_lcc2181"
output: github_document
---

Loading packages needed for Homework 3 and adding code to format plots.

```{r, message=FALSE}
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

## Problem 1

Loading instacart dataset for Problem 1

```{r}
data("instacart")
```

There are `r nrow(instacart)` observations and `r ncol(instacart)` variables in the Instacart dataset. Some key variables in the data set include `order_id`, `product_id`, `order_dow`, `product_name`, and `aisle`. `order_dow` describes what day of the week the order was placed. `product_name` describes the name of the product and `aisle` describes the name of the aisle. For example, the first observation in the dataset contains an order for Bulgarian Yogurt from the yogurt aisle.


How many aisles are there and which aisles are the most items ordered from?

*Using the `count` function to count the number of aisle_id's in the instacart data set, and the `distinct` function to remove duplicate aisle_ids to ensure all aisle_id's being counted are unique.
*Using the `group_by` function to group by aisle_id and `summarize` to summarize the total number of observations by aisle_id, assuming each observation in the dataset represents an item ordered. 
*Using `arrrange` to order in descending order, so that aisles will be listed in order most to least items ordered.

```{r}
count(distinct(instacart, aisle_id)) 

instacart %>% 
  group_by(aisle_id) %>% 
  summarize(number_of_items = n()) %>% 
  arrange(desc(number_of_items))

```

*There are 134 aisles. 
*The most items are ordered from aisle 83 (150609) followed by aisle 24 (150473).

Making a plot showing number of items ordered in each aisle. The plot is limited to aisles with more than 10000 items ordered.

*Using `group_by` to group by aisle_id and `summarize` to count the total number of observations by aisle_id, assuming each observation (row) represents an item ordered
*Using `filter` to restrict to aisles with more than 10,0000 items ordered
*Using `ggplot` to create a plot with aisle_id on the x-axis and number_of_items on the y-axis

```{r}
instacart %>% 
  group_by(aisle_id) %>% 
  summarize(number_of_items = n()) %>% 
  filter(number_of_items > 10000) %>% 
  ggplot(aes(x = aisle_id, y = number_of_items)) +
  geom_point()
```

Making a table showing the three most popular items in each of the aisles.

*Using `filter` to filter the data set to the three aisles - "baking ingredients", "dog food care", and "packaged vegetables fruits"
*Using `group_by` to group the observations by aisle and product_name
*Using `summarize` to count the total number of observations by aisle_id, assuming each observation (row) represents an item ordered
*Using `filter` to filter to those with a ranking of less than 4 (so, a ranking of 1, 2, or 3) as determined by using the `min_rank` function with descending n_items to ensure that those most number of items are ranked as highest
*Using `select` to ensure that the variables aisle, product_name, and n_items are included in the table
*Using `arrange` to order the table by aisle and most to least number of items in each aisle
*Using `knitr::kable()` to convert the table into a reader-friendly format

```{r}
instacart %>% 
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>% 
  summarize(n_items = n()) %>% 
  filter(min_rank(desc(n_items)) < 4) %>% 
  select(aisle, product_name, n_items) %>% 
  arrange(aisle, desc(n_items)) %>% 
  knitr::kable()
```

Making a table showing mean hour of day at which Pink Lady Apples and Coffee Ice Cream are ordered each day of week

*Using `filter` to limit the table to the products "Pink Lady Apples" and "Coffee Ice Cream"
*Using `mutate` to convert the order_dow variable into day names rather than numbers using the `recode` function, assuming that Sunday represents day 0.
*Using `group_by` to group by order_dow and product_name
*Using `summarize` to create a new variable mean_hour_of_day which takes the mean value of the order hour of day
*Using `pivot_wider` to convert format of table so that order_dow becomes columns taking on values from mean_hour_of_day variable created in the previous step
*Using `relocate` to order the table with product_name first followed by days of the week in chronological order
*Using `knitr::kable()` to convert the table into a reader-friendly format

```{r, message=FALSE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  mutate(order_dow = recode(order_dow,
    `0` = "Sunday", `1` = "Monday", `2` = "Tuesday", `3` = "Wednesday", 
    `4` = "Thursday", `5` = "Friday", `6` = "Saturday")) %>% 
  group_by(order_dow, product_name) %>%
  summarize(mean_hour_of_day = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_of_day
  ) %>%
  relocate(product_name, Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday) %>% 
  knitr::kable()
```

## Problem 2

Loading the BRFSS data for Problem 2

```{r}
data("brfss_smart2010")
```

Cleaning the BRFSS data and assigning to new data frame `brfss_df`:

*Using `janitor::clean_names()` to format the variable names appropriately
*Using `rename` to change the variable name "locationabbr" to "state" and "locationdesc" to "location"
*Using `filter` to focus only on the "Overall Health" topic
*Using `mutate` to convert response into a factor variable with 5 levels ordered from "Poor" to "Excellent"

```{r}
brfss_df =
  brfss_smart2010 %>% 
  janitor::clean_names() %>%
  rename(state = locationabbr, location = locationdesc) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, 
      levels = c("Poor", "Fair", "Good", "Very good", "Excellent")), 
      ordered = TRUE
  )
```

In 2002, which states were observed at 7 or more locations? What about in 2010?

*Using `filter` to filter to the correct year (2002 in the first code block and 2010 in the second)
*Using `group_by` to group by state and `summarize` to count location the number of distinct locations
*Using `filter` to filter to states with 7 or more locations

```{r}
brfss_df %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n_locations = n_distinct(location)) %>% 
  filter(n_locations >= 7)

brfss_df %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  summarize(n_locations = n_distinct(location)) %>% 
  filter(n_locations >= 7)
```

*In 2002, the states with 7 or more locations were CT, FL, MA, NC, NJ, and PA
*In 2010, the states with 7 or more locations were CA, CO, FL, MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA

Constructing a data set limited to "Excellent" responses, containing year, state, and a variable that averages data_value across locations within state. 
Making a spaghetti plot of this average value of time within a state.

*Filtering to only include response values of "Excellent"
*Using `group_by` to group by state and year and `summarize` to create a new variable avg_data_value that takes the mean data_value across locations within a state in a given year
*Creating `ggplot` with year on x-axis and avg_data_value on y-axis and grouping by state to show a different line for each state

```{r}
brfss_excellent =
  brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  summarize(avg_data_value = mean(data_value))

brfss_excellent %>% 
  ggplot(aes(x = year, y = avg_data_value, group = state)) +
  geom_line()
```

Making a two-panel plot (for the years 2006 and 2010) showing distribution of data_value for responses among locations in NY state

*Using `filter` to filter to only NY state and to include either years 2006 or 2010
*Creating `ggplot` with response on x-axis and data_value on y_axis, adding `geom_point()` to include points for each location, and adding `facet_grid` to split into a two-panel plot by year (2006 and 2010)

```{r}
brfss_df %>% 
  filter(state == "NY", year %in% c(2006, 2010)) %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_point() +
  facet_grid(. ~ year)
```


