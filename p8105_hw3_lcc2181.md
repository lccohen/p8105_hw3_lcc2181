p8105\_hw3\_lcc2181
================

Loading packages needed for Homework 3 and adding code to format plots.

``` r
library(tidyverse)
library(p8105.datasets)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
```

## Problem 1

Loading instacart dataset for Problem 1

``` r
data("instacart")
```

There are 1384617 observations and 15 variables in the Instacart
dataset. Some key variables in the data set include `order_id`,
`product_id`, `order_dow`, `product_name`, and `aisle`. `order_dow`
describes what day of the week the order was placed. `product_name`
describes the name of the product and `aisle` describes the name of the
aisle. For example, the first observation in the dataset contains an
order for Bulgarian Yogurt from the yogurt aisle.

How many aisles are there and which aisles are the most items ordered
from?

*Using the `count` function to count the number of aisle\_id’s in the
instacart data set, and the `distinct` function to remove duplicate
aisle\_ids to ensure all aisle\_id’s being counted are unique. *Using
the `group_by` function to group by aisle\_id and `summarize` to
summarize the total number of observations by aisle\_id, assuming each
observation in the dataset represents an item ordered. \*Using
`arrrange` to order in descending order, so that aisles will be listed
in order most to least items ordered.

``` r
count(distinct(instacart, aisle_id)) 
```

    ## # A tibble: 1 × 1
    ##       n
    ##   <int>
    ## 1   134

``` r
instacart %>% 
  group_by(aisle_id) %>% 
  summarize(number_of_items = n()) %>% 
  arrange(desc(number_of_items))
```

    ## # A tibble: 134 × 2
    ##    aisle_id number_of_items
    ##       <int>           <int>
    ##  1       83          150609
    ##  2       24          150473
    ##  3      123           78493
    ##  4      120           55240
    ##  5       21           41699
    ##  6      115           36617
    ##  7       84           32644
    ##  8      107           31269
    ##  9       91           26240
    ## 10      112           23635
    ## # … with 124 more rows

*There are 134 aisles. *The most items are ordered from aisle 83
(150609) followed by aisle 24 (150473).

Making a plot showing number of items ordered in each aisle. The plot is
limited to aisles with more than 10000 items ordered.

*Using `group_by` to group by aisle\_id and `summarize` to count the
total number of observations by aisle\_id, assuming each observation
(row) represents an item ordered *Using `filter` to restrict to aisles
with more than 10,0000 items ordered \*Using `ggplot` to create a plot
with aisle\_id on the x-axis and number\_of\_items on the y-axis

``` r
instacart %>% 
  group_by(aisle_id) %>% 
  summarize(number_of_items = n()) %>% 
  filter(number_of_items > 10000) %>% 
  ggplot(aes(x = aisle_id, y = number_of_items)) +
  geom_point()
```

<img src="p8105_hw3_lcc2181_files/figure-gfm/unnamed-chunk-4-1.png" width="90%" />

Making a table showing the three most popular items in each of the
aisles.

*Using `filter` to filter the data set to the three aisles - “baking
ingredients”, “dog food care”, and “packaged vegetables fruits” *Using
`group_by` to group the observations by aisle and product\_name *Using
`summarize` to count the total number of observations by aisle\_id,
assuming each observation (row) represents an item ordered *Using
`filter` to filter to those with a ranking of less than 4 (so, a ranking
of 1, 2, or 3) as determined by using the `min_rank` function with
descending n\_items to ensure that those most number of items are ranked
as highest *Using `select` to ensure that the variables aisle,
product\_name, and n\_items are included in the table *Using `arrange`
to order the table by aisle and most to least number of items in each
aisle \*Using `knitr::kable()` to convert the table into a
reader-friendly format

``` r
instacart %>% 
  filter(
    aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle, product_name) %>% 
  summarize(n_items = n()) %>% 
  filter(min_rank(desc(n_items)) < 4) %>% 
  select(aisle, product_name, n_items) %>% 
  arrange(aisle, desc(n_items)) %>% 
  knitr::kable()
```

    ## `summarise()` has grouped output by 'aisle'. You can override using the `.groups` argument.

| aisle                      | product\_name                                 | n\_items |
|:---------------------------|:----------------------------------------------|---------:|
| baking ingredients         | Light Brown Sugar                             |      499 |
| baking ingredients         | Pure Baking Soda                              |      387 |
| baking ingredients         | Cane Sugar                                    |      336 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |       30 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |       28 |
| dog food care              | Small Dog Biscuits                            |       26 |
| packaged vegetables fruits | Organic Baby Spinach                          |     9784 |
| packaged vegetables fruits | Organic Raspberries                           |     5546 |
| packaged vegetables fruits | Organic Blueberries                           |     4966 |

Making a table showing mean hour of day at which Pink Lady Apples and
Coffee Ice Cream are ordered each day of week

*Using `filter` to limit the table to the products “Pink Lady Apples”
and “Coffee Ice Cream” *Using `mutate` to convert the order\_dow
variable into day names rather than numbers using the `recode` function,
assuming that Sunday represents day 0. *Using `group_by` to group by
order\_dow and product\_name *Using `summarize` to create a new variable
mean\_hour\_of\_day which takes the mean value of the order hour of day
*Using `pivot_wider` to convert format of table so that order\_dow
becomes columns taking on values from mean\_hour\_of\_day variable
created in the previous step *Using `relocate` to order the table with
product\_name first followed by days of the week in chronological order
\*Using `knitr::kable()` to convert the table into a reader-friendly
format

``` r
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  mutate(order_dow = recode(order_dow,
    `0` = "Sunday", `1` = "Monday", `2` = "Tuesday", `3` = "Wednesday", 
    `4` = "Thursday", `5` = "Friday", `6` = "Saturday")) %>% 
  group_by(order_dow, product_name) %>%
  summarize(mean_hour_of_day = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_of_day
  ) %>%
  relocate(product_name, Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday) %>% 
  knitr::kable()
```

| product\_name    |   Sunday |   Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday |
|:-----------------|---------:|---------:|---------:|----------:|---------:|---------:|---------:|
| Coffee Ice Cream | 13.77419 | 14.31579 | 15.38095 |  15.31818 | 15.21739 | 12.26316 | 13.83333 |
| Pink Lady Apples | 13.44118 | 11.36000 | 11.70213 |  14.25000 | 11.55172 | 12.78431 | 11.93750 |

## Problem 2

Loading the BRFSS data for Problem 2

``` r
data("brfss_smart2010")
```

Cleaning the BRFSS data and assigning to new data frame `brfss_df`:

*Using `janitor::clean_names()` to format the variable names
appropriately *Using `rename` to change the variable name “locationabbr”
to “state” and “locationdesc” to “location” *Using `filter` to focus
only on the “Overall Health” topic *Using `mutate` to convert response
into a factor variable with 5 levels ordered from “Poor” to “Excellent”

``` r
brfss_df =
  brfss_smart2010 %>% 
  janitor::clean_names() %>%
  rename(state = locationabbr, location = locationdesc) %>% 
  filter(topic == "Overall Health") %>% 
  mutate(response = factor(response, 
      levels = c("Poor", "Fair", "Good", "Very good", "Excellent")), 
      ordered = TRUE
  )
```

In 2002, which states were observed at 7 or more locations? What about
in 2010?

*Using `filter` to filter to the correct year (2002 in the first code
block and 2010 in the second) *Using `group_by` to group by state and
`summarize` to count location the number of distinct locations \*Using
`filter` to filter to states with 7 or more locations

``` r
brfss_df %>% 
  filter(year == 2002) %>% 
  group_by(state) %>% 
  summarize(n_locations = n_distinct(location)) %>% 
  filter(n_locations >= 7)
```

    ## # A tibble: 6 × 2
    ##   state n_locations
    ##   <chr>       <int>
    ## 1 CT              7
    ## 2 FL              7
    ## 3 MA              8
    ## 4 NC              7
    ## 5 NJ              8
    ## 6 PA             10

``` r
brfss_df %>% 
  filter(year == 2010) %>% 
  group_by(state) %>% 
  summarize(n_locations = n_distinct(location)) %>% 
  filter(n_locations >= 7)
```

    ## # A tibble: 14 × 2
    ##    state n_locations
    ##    <chr>       <int>
    ##  1 CA             12
    ##  2 CO              7
    ##  3 FL             41
    ##  4 MA              9
    ##  5 MD             12
    ##  6 NC             12
    ##  7 NE             10
    ##  8 NJ             19
    ##  9 NY              9
    ## 10 OH              8
    ## 11 PA              7
    ## 12 SC              7
    ## 13 TX             16
    ## 14 WA             10

*In 2002, the states with 7 or more locations were CT, FL, MA, NC, NJ,
and PA *In 2010, the states with 7 or more locations were CA, CO, FL,
MA, MD, NC, NE, NJ, NY, OH, PA, SC, TX, and WA

Constructing a data set limited to “Excellent” responses, containing
year, state, and a variable that averages data\_value across locations
within state. Making a spaghetti plot of this average value of time
within a state.

*Filtering to only include response values of “Excellent” *Using
`group_by` to group by state and year and `summarize` to create a new
variable avg\_data\_value that takes the mean data\_value across
locations within a state in a given year \*Creating `ggplot` with year
on x-axis and avg\_data\_value on y-axis and grouping by state to show a
different line for each state

``` r
brfss_excellent =
  brfss_df %>% 
  filter(response == "Excellent") %>% 
  group_by(state, year) %>% 
  summarize(avg_data_value = mean(data_value))
```

    ## `summarise()` has grouped output by 'state'. You can override using the `.groups` argument.

``` r
brfss_excellent %>% 
  ggplot(aes(x = year, y = avg_data_value, group = state)) +
  geom_line()
```

    ## Warning: Removed 3 row(s) containing missing values (geom_path).

<img src="p8105_hw3_lcc2181_files/figure-gfm/unnamed-chunk-10-1.png" width="90%" />

Making a two-panel plot (for the years 2006 and 2010) showing
distribution of data\_value for responses among locations in NY state

*Using `filter` to filter to only NY state and to include either years
2006 or 2010 *Creating `ggplot` with response on x-axis and data\_value
on y\_axis, adding `geom_point()` to include points for each location,
and adding `facet_grid` to split into a two-panel plot by year (2006 and
2010)

``` r
brfss_df %>% 
  filter(state == "NY", year %in% c(2006, 2010)) %>% 
  ggplot(aes(x = response, y = data_value)) +
  geom_point() +
  facet_grid(. ~ year)
```

<img src="p8105_hw3_lcc2181_files/figure-gfm/unnamed-chunk-11-1.png" width="90%" />

## Problem 3

Loading, tidying and wrangling the data for use in Problem 3

*Using `read_csv` to load in the data file *Including a weekday
vs. weekend variable by using `mutate` to create new variable
weekday\_vs\_weekend which is created using `recode` to code days of the
week *Using `pivot_longer` to display activity count at each minute as a
new observation (row) rather than as a variable (column) *Using mutate
to convert new `minute` variable to numeric

``` r
accel_df =
  read_csv("data/accel_data.csv", show_col_types = FALSE) %>%
  mutate(
    weekday_vs_weekend = recode(day,
      "Monday" = "Weekday", "Tuesday" = "Weekday",  "Wednesday" = "Weekday",
      "Thursday" = "Weekday",  "Friday" = "Weekday", "Saturday" = "Weekend", 
      "Sunday" = "Weekend")
  ) %>% 
  pivot_longer (
    activity.1:activity.1440,
    names_to = "minute",
    names_prefix = "activity.",
    values_to = "activity"
  ) %>% 
  mutate(minute = as.numeric(minute))
```

The resulting dataset has 50400 observations and 6 columns. The
variables are week, day\_id, day, weekday\_vs\_weekend, minute, and
activity. There is an observation recording activity counts for each
minute of each day over the course of 35 days.

Creating a total activity variable for each day, aggregating across
minutes \*Using `group_by` to group by day and `summarize` to create a
new variable total\_activity which is sum of all activity observations
from each minute of the day.

``` r
accel_df %>% 
  group_by(day_id) %>% 
  summarize(total_activity = sum(activity))
```

    ## # A tibble: 35 × 2
    ##    day_id total_activity
    ##     <dbl>          <dbl>
    ##  1      1        480543.
    ##  2      2         78828.
    ##  3      3        376254 
    ##  4      4        631105 
    ##  5      5        355924.
    ##  6      6        307094.
    ##  7      7        340115.
    ##  8      8        568839 
    ##  9      9        295431 
    ## 10     10        607175 
    ## # … with 25 more rows

The individual’s activity counts vary substantially by day. His highest
activity count is reported on day 16 (685910.00) and his lowest total
activity count is reported on days 24 and 31 (1440.00).

Making a single-panel plot that shows the 24-hour activity time courses
each day, using color to indicate day of the week. *Using `mutate` to
divide the minute variable by 60 in order to convert minutes into hours
*Creating a `ggplot` with hour on the x-axis and activity on the y-axis,
colored by day of the week; adding `geom_smooth()` to create a line for
each day.

``` r
accel_df %>% 
  mutate(hour = minute/60) %>% 
  ggplot(aes(x = hour, y = activity, group = day_id, color = day)) +
  geom_line() +
  labs(
    title = "24 Hour Activity Time Course For Each Day",
    x = "Hour of the Day",
    y = "Activity"
  ) +
  scale_x_continuous(breaks = c(0, 4, 8, 12, 16, 20, 24))
```

<img src="p8105_hw3_lcc2181_files/figure-gfm/unnamed-chunk-14-1.png" width="90%" />
Based on this graph, I can tell the day of week and hour of day the
individual had the highest activity count. Specifically, the individual
appears to have been most active on a Wednesday in the 19th hour of the
day.
